[parser]
encoder = 'bert'            # Options: 'lstm', 'bert'
feat = 'bert'               # Options: 'char', 'tag', 'bert'
punct = True                # Include punctuation
buckets = 32                # Bucket size
max_len = 150               # Max sentence length

[training]
optimizer = 'adam'          # Options: 'adam', 'sgd'
lr = 1e-3                   # Learning rate
clip = 5.0                  # Gradient clipping
epochs = 30                 # Number of training epochs
batch_size = 64             # Batch size
decay = 0.75                # Learning rate decay factor (e.g., 0.75 reduces LR by 25)
decay_steps = 5000          # Number of steps between decays
